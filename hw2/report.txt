(1) lan1.hmm test1.seq
------------------------------------
Viterbi using direct probabilities
Viterbi  MLE log prob = -1.407294E+01
------------------------------------
Viterbi using log probabilities
Viterbi  MLE log prob = -1.407294E+01
------------------------------------


(2)lan2.hmm test1.seq
------------------------------------
Viterbi using direct probabilities
Viterbi  MLE log prob = -2.078765E+01
------------------------------------
Viterbi using log probabilities
Viterbi  MLE log prob = -2.078765E+01
------------------------------------


(3)lan1.hmm test2.seq
------------------------------------
Viterbi using direct probabilities
Viterbi  MLE log prob = -2.097969E+01
------------------------------------
Viterbi using log probabilities
Viterbi  MLE log prob = -2.097969E+01
------------------------------------


(4)lan2.hmm test2.seq
------------------------------------
Viterbi using direct probabilities
Viterbi  MLE log prob = -1.449021E+01
------------------------------------
Viterbi using log probabilities
Viterbi  MLE log prob = -1.449021E+01
------------------------------------


Conclusion: for test sentence "c i n", the first language model have higher log likelyhood;
for test sentence "n i c", the second language model have higher log likely hood.
it makes sense because in the first traning set, sequence "c i" appears frequently, but "i c" never appears;
while in the second training set, sequence "i c" frequently but "c i" never appears.
